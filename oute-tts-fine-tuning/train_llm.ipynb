{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9a7c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/llm-fine-tuning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce GTX 1660 SUPER. Num GPUs = 1. Max memory: 5.606 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "dtype = None\n",
    "load_in_4bit = False \n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"OuteAI/Llama-OuteTTS-1.0-1B\",\n",
    "    max_seq_length= max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf61ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "#LoRA does not work with float32 only works with bfloat16 !!!\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, \n",
    "    target_modules = [\"q_proj\", \"v_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False, \n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022ca905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Audio,Dataset\n",
    "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=24000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25d4ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1195/1195 [00:02<00:00, 407.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9d13b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,195 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 13,631,488/1,262,028,800 (1.08% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.856700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.773200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>4.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>4.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.952700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.457200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>4.068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.729100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.762100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a34f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/adrian/anaconda3/envs/llm-fine-tuning/lib/python3.11/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogitsProcessor\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeneration_utils\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm-fine-tuning/lib/python3.11/site-packages/torchaudio/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize extension and backend first\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_backend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[32m      4\u001b[39m     AudioMetaData,\n\u001b[32m      5\u001b[39m     get_audio_backend,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     set_audio_backend,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     14\u001b[39m     compliance,\n\u001b[32m     15\u001b[39m     datasets,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     utils,\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm-fine-tuning/lib/python3.11/site-packages/torchaudio/_extension/__init__.py:38\u001b[39m\n\u001b[32m     36\u001b[39m _IS_ALIGN_AVAILABLE = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlibtorchaudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     42\u001b[39m     _check_cuda_version()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm-fine-tuning/lib/python3.11/site-packages/torchaudio/_extension/utils.py:60\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m(lib)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm-fine-tuning/lib/python3.11/site-packages/torch/_ops.py:1357\u001b[39m, in \u001b[36mload_library\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimport_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module):\n\u001b[32m   1348\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1349\u001b[39m \u001b[33;03m    Imports a Python module that has torch.library registrations.\u001b[39;00m\n\u001b[32m   1350\u001b[39m \n\u001b[32m   1351\u001b[39m \u001b[33;03m    Generally, to extend PyTorch with custom operators, a user will\u001b[39;00m\n\u001b[32m   1352\u001b[39m \u001b[33;03m    create a Python module whose import triggers registration of\u001b[39;00m\n\u001b[32m   1353\u001b[39m \u001b[33;03m    the custom operators via a torch.ops.load_library call or a call\u001b[39;00m\n\u001b[32m   1354\u001b[39m \u001b[33;03m    to one or more torch.library.* APIs.\u001b[39;00m\n\u001b[32m   1355\u001b[39m \n\u001b[32m   1356\u001b[39m \u001b[33;03m    It is unexpected for Python modules to have side effects, so some\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m \u001b[33;03m    linters and formatters will complain. Use this API to import Python\u001b[39;00m\n\u001b[32m   1358\u001b[39m \u001b[33;03m    modules that contain these torch.library side effects.\u001b[39;00m\n\u001b[32m   1359\u001b[39m \n\u001b[32m   1360\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   1361\u001b[39m \u001b[33;03m        module (str): The name of the Python module to import\u001b[39;00m\n\u001b[32m   1362\u001b[39m \n\u001b[32m   1363\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1364\u001b[39m     importlib.import_module(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm-fine-tuning/lib/python3.11/ctypes/__init__.py:376\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: /home/adrian/anaconda3/envs/llm-fine-tuning/lib/python3.11/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "import torchaudio.transforms as T\n",
    "from transformers import LogitsProcessor\n",
    "import transformers.generation.utils as generation_utils\n",
    "from transformers import AutoModelForCausalLM\n",
    "import re\n",
    "\n",
    "def get_audio(tokens):\n",
    "        decoded_output = tokenizer.batch_decode(tokens, skip_special_tokens=False)[0]\n",
    "        c1 = list(map(int,re.findall(r\"<\\|c1_(\\d+)\\|>\", decoded_output)))\n",
    "        c2 = list(map(int,re.findall(r\"<\\|c2_(\\d+)\\|>\", decoded_output)))\n",
    "\n",
    "        t = min(len(c1), len(c2))\n",
    "        c1 = c1[:t]\n",
    "        c2 = c2[:t]\n",
    "        output = [c1,c2]\n",
    "        if not output:\n",
    "            print(\"No audio tokens found in the output\")\n",
    "            return None\n",
    "\n",
    "        return data_processor.audio_processor.audio_codec.decode(\n",
    "            torch.tensor([output], dtype=torch.int64).to(data_processor.audio_processor.audio_codec.device)\n",
    "        )\n",
    "\n",
    "class RepetitionPenaltyLogitsProcessorPatch(LogitsProcessor):\n",
    "    def __init__(self, penalty: float):\n",
    "        penalty_last_n = 64\n",
    "        print(\"ðŸ”„ Using patched RepetitionPenaltyLogitsProcessor -> RepetitionPenaltyLogitsProcessorPatch | penalty_last_n: {penalty_last_n}\")\n",
    "        if penalty_last_n is not None:\n",
    "            if not isinstance(penalty_last_n, int) or penalty_last_n < 0:\n",
    "                raise ValueError(f\"`penalty_last_n` has to be a non-negative integer, but is {penalty_last_n}\")\n",
    "        if not isinstance(penalty, float) or penalty <= 0:\n",
    "            raise ValueError(f\"`penalty` has to be a positive float, but is {penalty}\")\n",
    "\n",
    "        self.penalty_last_n = penalty_last_n\n",
    "        self.penalty = penalty\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor`):\n",
    "                Indices of input sequence tokens in the vocabulary (shape `(batch_size, sequence_length)`).\n",
    "            scores (`torch.FloatTensor`):\n",
    "                Prediction scores of a language modeling head (shape `(batch_size, vocab_size)`).\n",
    "\n",
    "        Returns:\n",
    "            `torch.FloatTensor`: The modified prediction scores.\n",
    "        \"\"\"\n",
    "        # Check if penalties should be applied\n",
    "        if self.penalty_last_n == 0 or self.penalty == 1.0:\n",
    "            return scores\n",
    "\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        vocab_size = scores.shape[-1]\n",
    "\n",
    "        # Process each batch item independently\n",
    "        for b in range(batch_size):\n",
    "            # 1. Determine the penalty window\n",
    "            start_index = max(0, seq_len - self.penalty_last_n)\n",
    "            window_indices = input_ids[b, start_index:] # Shape: (window_len,)\n",
    "\n",
    "            if window_indices.numel() == 0: # Skip if window is empty\n",
    "                continue\n",
    "\n",
    "            # 2. Find unique tokens within the window\n",
    "            tokens_in_window = set(window_indices.tolist())\n",
    "\n",
    "            # 3. Apply repetition penalty to the scores for this batch item\n",
    "            for token_id in tokens_in_window:\n",
    "                if token_id >= vocab_size:\n",
    "                    continue\n",
    "\n",
    "                logit = scores[b, token_id]\n",
    "\n",
    "                if logit <= 0:\n",
    "                    logit *= self.penalty\n",
    "                else:\n",
    "                    logit /= self.penalty\n",
    "\n",
    "                # Update the score\n",
    "                scores[b, token_id] = logit\n",
    "\n",
    "        return scores\n",
    "\n",
    "generation_utils.RepetitionPenaltyLogitsProcessor = RepetitionPenaltyLogitsProcessorPatch\n",
    "AutoModelForCausalLM.generate = generation_utils.GenerationMixin.generate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    formated_text = \"<|text_start|>\"+input_text+\"<|text_end|>\"\n",
    "    prompt = \"\\n\".join([\n",
    "        \"<|im_start|>\",\n",
    "        formated_text,\n",
    "        \"<|audio_start|><|global_features_start|>\",\n",
    "    ])\n",
    "\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    print(\"Generating token sequence...\")\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=0.4,\n",
    "        top_k=40,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        min_p=0.05,\n",
    "        max_new_tokens=2048, # Limit generation length\n",
    "    )\n",
    "    print(\"Token sequence generated.\")\n",
    "\n",
    "\n",
    "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
    "    audio = get_audio(generated_ids)\n",
    "    audio = audio.cpu()\n",
    "    from IPython.display import Audio, display\n",
    "    display(Audio(audio.squeeze(0), rate=24000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
